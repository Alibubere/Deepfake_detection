{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b94e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507746d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDatasetwithCV(Dataset):\n",
    "\n",
    "    def __init__(self,root_dir,mode=\"RGB\",transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode.lower()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.class_to_idx = {}\n",
    "        self.samples = []\n",
    "\n",
    "        classes = sorted(os.listdir(root_dir))\n",
    "        for idx , cls in enumerate(classes):\n",
    "\n",
    "            self.class_to_idx[cls] = idx\n",
    "\n",
    "            cls_dir = os.path.join(root_dir,cls)\n",
    "\n",
    "            for file in os.listdir(cls_dir):\n",
    "                self.samples.append((os.path.join(cls_dir,file),idx))\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path , label = self.samples[index]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.mode == \"gray\":\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "            img = img[...,None]\n",
    "\n",
    "        elif self.mode == \"clahe\":\n",
    "            lab = cv2.cvtColor(img,cv2.COLOR_RGB2LAB)\n",
    "            l , a , b = cv2.split(lab)\n",
    "            clahe = cv2.createCLAHE(2.0,(8,8))\n",
    "            l = clahe.apply(l)\n",
    "            img = cv2.merge((l,a,b))\n",
    "            img = cv2.cvtColor(img,cv2.COLOR_Lab2RGB)\n",
    "\n",
    "        elif self.mode == \"edges\":\n",
    "            gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "            img = cv2.Canny(gray,100,200)[...,None]\n",
    "\n",
    "        \n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = torch.from_numpy(img)\n",
    "\n",
    "        if img.ndim == 2:\n",
    "            img = img[...,None]\n",
    "\n",
    "        img.permute(2,0,1)\n",
    "\n",
    "        if img.shape[0] == 1:\n",
    "            img = img.repeat(3,1,1)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        \n",
    "        return img , label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477efea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
